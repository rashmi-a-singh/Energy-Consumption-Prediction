# -*- coding: utf-8 -*-
"""Energy Consumption.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q92OsVomQPuF-maxAP-IkJA6kT-lj4DQ
"""

# --- Part 1: Gathering Our Ingredients ---

import pandas as pd
import warnings
warnings.filterwarnings('ignore')

print("Attempting to load the data files...")

# Try to load the three essential CSV files
try:
    # Load the energy usage data
    train_df = pd.read_csv('train.csv')
    print("‚úÖ Successfully loaded: train.csv")

    # Load the information about each building
    building_df = pd.read_csv('building_metadata.csv')
    print("‚úÖ Successfully loaded: building_metadata.csv")

    # Load the weather data
    weather_train_df = pd.read_csv('weather_train.csv')
    print("‚úÖ Successfully loaded: weather_train.csv")

    print("\nLet's look at the first few lines of each file:")
    print("\n--- Energy Data (train.csv) ---")
    print(train_df.head())

    print("\n--- Building Data (building_metadata.csv) ---")
    print(building_df.head())

    print("\n--- Weather Data (weather_train.csv) ---")
    print(weather_train_df.head())

except FileNotFoundError as e:
    print(f"\n‚ùå ERROR: Could not find a file.")
    print(f"Please make sure 'train.csv', 'building_metadata.csv', and 'weather_train.csv' are in the same folder as your script.")

# --- Part 2: Combining Our Clues ---

# Make sure you have run the code from Part 1 first!

print("Starting to combine the three data tables...")

# First, merge the energy data with the building data.
# We connect them using the 'building_id' column, which exists in both tables.
print("Step 1: Merging energy data with building info...")
# Using a 'left' merge means we keep every row from the energy data.
temp_df = train_df.merge(building_df, on='building_id', how='left')

# Now, merge the result with the weather data.
# This time, we need to match both the location ('site_id') and the time ('timestamp').
print("Step 2: Merging the result with weather data...")
full_train_df = temp_df.merge(weather_train_df, on=['site_id', 'timestamp'], how='left')

print("\n‚úÖ All data has been combined into one master table!")

# Let's see how big our new table is
# The first number is rows, the second is columns.
print(f"The new master table has {full_train_df.shape[0]} rows and {full_train_df.shape[1]} columns.")

print("\nHere's a look at the first few rows of our combined table:")
# Notice how it now has columns from all three original files!
print(full_train_df.head())

# --- Part 3: Focusing on One Story & Adding Clues ---

# Make sure you have run the code from Parts 1 and 2 first!

print("The full dataset is very large. Let's focus on just one building to start.")

# We'll pick Building #0 for our analysis.
BUILDING_ID = 105
df = full_train_df[full_train_df['building_id'] == BUILDING_ID].copy()

print(f"‚úÖ Successfully filtered for Building ID: {BUILDING_ID}")

# It's helpful to use the 'timestamp' as the main label for our rows.
# First, convert the 'timestamp' column to a proper date/time format.
df['timestamp'] = pd.to_datetime(df['timestamp'])
df.set_index('timestamp', inplace=True)

print("üßπ Cleaning up missing values...")
# Some weather data might be missing. We'll fill the gaps using a simple method.
df.interpolate(method='linear', limit_direction='both', inplace=True)
print("‚úÖ Missing values handled.")

print("‚ú® Creating new time-based clues (features)...")
# Let's extract useful information from the timestamp index.
df['hour'] = df.index.hour
df['dayofweek'] = df.index.dayofweek # Monday=0, Sunday=6
df['month'] = df.index.month
df['dayofyear'] = df.index.dayofyear
df['weekofyear'] = df.index.isocalendar().week.astype(int)
print("‚úÖ New features created: hour, dayofweek, month, dayofyear, weekofyear")


print("\nHere's what our data for Building 0 looks like now:")
# Notice the new columns on the far right!
print(df.head())

# --- Part 4: Training Our First Prediction Model ---

# Make sure you have run all previous parts first!
from sklearn.model_selection import train_test_split
import xgboost as xgb

print("Preparing the 'study materials' for our model...")

# These are the 'clues' we'll give the model to learn from.
FEATURES = [
    'square_feet', 'year_built', 'air_temperature', 'cloud_coverage',
    'dew_temperature', 'hour', 'dayofweek', 'month', 'dayofyear', 'weekofyear'
]

# This is the 'answer' we want the model to predict.
TARGET = 'meter_reading'

# Separate our clues (X) from the answer (y).
X = df[FEATURES]
y = df[TARGET]

print(f"‚úÖ Features to be used: {FEATURES}")
print(f"‚úÖ Target to be predicted: {TARGET}")


print("\nSplitting the data into a 'study' set and an 'exam' set...")
# We will use the first part of the data for training and the last part for testing.
# `shuffle=False` is important for time-series data. We don't want to mix up the timeline.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

print(f"‚úÖ Data split complete.")
print(f"   - Study set size: {len(X_train)} rows")
print(f"   - Exam set size: {len(X_test)} rows")


print("\nüß† Now, let the learning begin! Training the XGBoost model...")
# Create an instance of the XGBoost Regressor model.
# The parameters are set to be effective but also run quickly.
model = xgb.XGBRegressor(
    n_estimators=1000,      # We'll build 1000 simple models.
    learning_rate=0.05,     # How fast the model learns.
    objective='reg:squarederror',
    early_stopping_rounds=50 # Stop if the model doesn't improve for 50 rounds.
)

# Fit the model to our 'study' data.
# We also provide the 'exam' data so the model can check its own progress.
model.fit(X_train, y_train,
          eval_set=[(X_train, y_train), (X_test, y_test)],
          verbose=100) # This will print an update every 100 rounds of training.

print("\nüéâ Model training complete! Our model is now ready to make predictions.")

# --- Part 5: Grading the Exam and Visualizing the Results ---

# Make sure you have run all previous parts first!
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
import numpy as np

# 1. Ask the model to make predictions on the 'exam' data
print("üìä Making predictions on the 'exam' data...")
predictions = model.predict(X_test)

# 2. Calculate the error score to see how well it did
# The Root Mean Squared Error (RMSE) is a standard way to measure the error of a model.
rmse = np.sqrt(mean_squared_error(y_test, predictions))
print(f"‚úÖ Prediction complete.")
print(f"   - The model's error score (RMSE) is: {rmse:.4f}")
print("   (A lower score is better!)")

# 3. Add the predictions to our test data so we can plot them easily
# We'll put the actual values and the predicted values into one table.
results_df = y_test.to_frame()
results_df['prediction'] = predictions

# 4. Create the plot to visualize the results
print("\nüìà Generating a plot to compare actual vs. predicted energy use...")
plt.figure(figsize=(15, 6)) # Make the plot nice and wide

# Plot the actual, real energy values (the 'ground truth')
plt.plot(results_df.index, results_df['meter_reading'], label='Actual Energy Use', color='blue', linewidth=2)

# Plot the model's predictions to see how they line up
plt.plot(results_df.index, results_df['prediction'], label='Predicted Energy Use', color='red', linestyle='--')

# Add labels and a title to make the plot clear
plt.title(f'Actual vs. Predicted Energy Use for Building {BUILDING_ID}')
plt.xlabel('Date')
plt.ylabel('Meter Reading')
plt.legend()
plt.grid(True)
plt.show()

print("\nüéâ Project complete! You've successfully trained a machine learning model and visualized its predictions.")